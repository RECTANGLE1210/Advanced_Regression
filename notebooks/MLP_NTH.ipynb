{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.getcwd().replace('/notebooks', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/preprocessed/train_preprocessed.csv')\n",
    "data_array = df.values\n",
    "data_tensor = torch.tensor(data_array, dtype=torch.float32)\n",
    "\n",
    "print(data_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(df.iloc[:, :-2].values, dtype=torch.float32)  # Tất cả cột trừ cột cuối\n",
    "labels = torch.tensor(df.iloc[:, -1].values, dtype=torch.float32).view(-1, 1)  # Cột cuối cùng\n",
    "\n",
    "print(features.shape, labels.shape)  # Kiểm tra kích thước tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=75):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64,32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kích thước batch\n",
    "batch_size = 32  \n",
    "\n",
    "# Chia tập train (80%) và validation (20%)\n",
    "train_size = int(0.8 * len(features))\n",
    "val_size = len(features) - train_size\n",
    "\n",
    "# Chia dataset thật thành train và validation\n",
    "train_dataset, val_dataset = random_split(TensorDataset(features, labels), [train_size, val_size])\n",
    "\n",
    "# Tạo DataLoader cho train và validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Kiểm tra một batch train\n",
    "for batch_features, batch_labels in train_loader:\n",
    "    print(\"Train Batch Shape:\", batch_features.shape, batch_labels.shape)\n",
    "    break  # Chỉ in thử batch đầu tiên\n",
    "\n",
    "# Kiểm tra một batch validation\n",
    "for batch_features, batch_labels in test_loader:\n",
    "    print(\"Validation Batch Shape:\", batch_features.shape, batch_labels.shape)\n",
    "    break  # Chỉ in thử batch đầu tiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    num_batches = 0  # Đếm số batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            num_batches += 1  # Cập nhật số batch\n",
    "\n",
    "    # Trả về loss trung bình thay vì tổng loss\n",
    "    return test_loss / num_batches if num_batches > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo mô hình và chuyển lên device\n",
    "model = MLP()\n",
    "summary(model, input_size = (75,))\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách lưu loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "batch_losses = []  # Lưu loss theo từng batch để minh họa chi tiết hơn\n",
    "\n",
    "max_epoch = 200\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(max_epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    batch_loss_list = []  # Lưu loss của từng batch trong epoch\n",
    "\n",
    "    for i, (features, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epoch}\")):\n",
    "        # Chuyển dữ liệu lên device\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Fix lỗi thiếu dấu ()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        batch_loss_list.append(loss.item())  # Ghi lại loss từng batch\n",
    "\n",
    "    # Tính trung bình loss trên tập train\n",
    "    epoch_loss = running_loss / (i + 1)\n",
    "    train_losses.append(epoch_loss)\n",
    "    batch_losses.append(batch_loss_list)  # Ghi lại loss từng batch theo epoch\n",
    "\n",
    "    # Đánh giá trên tập test\n",
    "    model.eval()\n",
    "    test_loss = evaluate(model, test_loader, criterion)\n",
    "    scheduler.step(test_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{max_epoch} - Train Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vẽ loss theo epoch\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Vẽ Train & Validation Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, max_epoch+1), train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "plt.plot(range(1, max_epoch+1), test_losses, label=\"Test Loss\", color=\"red\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for epoch in [0, max_epoch//4, max_epoch//2, max_epoch-1]:  # Chọn vài epoch để hiển thị\n",
    "    plt.plot(batch_losses[epoch], label=f\"Epoch {epoch+1}\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss per Batch in Selected Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join('data', 'raw', 'test.csv')\n",
    "data = pd.read_csv(filepath, index_col='Id')\n",
    "\n",
    "# Loại bỏ cột cuối cùng khỏi danh sách cột xử lý\n",
    "columns_to_process = data.columns\n",
    "\n",
    "# Find columns with missing values\n",
    "lst_of_missing = [col for col in columns_to_process if data[col].isnull().sum() > 0]\n",
    "\n",
    "# Find numerical and categorical columns\n",
    "lst_of_numerical = [col for col in columns_to_process if data[col].dtype != 'object']\n",
    "lst_of_categorical = [col for col in columns_to_process if data[col].dtype == 'object']\n",
    "\n",
    "# Print information about missing categorical columns\n",
    "cat_missing = set(lst_of_categorical) & set(lst_of_missing)\n",
    "print(f'There are {len(cat_missing)} categorical columns with missing values')\n",
    "for col in cat_missing:\n",
    "    print(f'{col:<13}: {data[col].isnull().sum(): <4} missing values - {data[col].isnull().sum() / len(data) * 100:.2f}% - {len(data[col].unique())} unique values')\n",
    "\n",
    "# Save preprocessed data\n",
    "output_dir = os.path.join('data', 'preprocessed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_filepath = os.path.join(output_dir, 'test_preprocessed.csv')\n",
    "data.to_csv(output_filepath)\n",
    "print(f\"Preprocessed data saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './data/preprocessed/test_preprocessed.csv'\n",
    "data = pd.read_csv(filepath, index_col='Id')\n",
    "\n",
    "# Fill missing values\n",
    "data['LotFrontage'] = data['LotFrontage'].fillna(data[data['LotFrontage'] < 300]['LotFrontage'].mean())\n",
    "data['GarageYrBlt'] = data['GarageYrBlt'].fillna(data['GarageYrBlt'].interpolate())\n",
    "data['MasVnrArea'] = data['MasVnrArea'].fillna(0)\n",
    "\n",
    "# Fill missing values for categorical columns\n",
    "data['MasVnrType'] = data['MasVnrType'].fillna('None')\n",
    "data.drop(['MiscFeature', 'PoolQC', 'Fence', 'Alley'], axis=1, inplace=True)\n",
    "\n",
    "# Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le  # Store encoders for potential inverse transform\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "data_standardized = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "\n",
    "# Save preprocessed data\n",
    "output_filepath = './data/preprocessed/test_preprocessed.csv'\n",
    "data_standardized.to_csv(output_filepath)\n",
    "print(f\"Preprocessed data saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed test data\n",
    "test_filepath = './data/preprocessed/test_preprocessed.csv'\n",
    "test_data = pd.read_csv(test_filepath, index_col='Id')\n",
    "\n",
    "test_tensor = torch.tensor(test_data.values, dtype=torch.float32)\n",
    "\n",
    "# Load trained model\n",
    "model_path = './model/model.pth'\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    test_tensor = test_tensor.to(device)\n",
    "    predictions = model(test_tensor).cpu().numpy()\n",
    "\n",
    "# Load scaling parameters\n",
    "# Load scaling parameters safely\n",
    "scaling_params_filepath = './data/preprocessed/scaling_params.txt'\n",
    "\n",
    "with open(scaling_params_filepath, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    mean = float(lines[0].split()[1])\n",
    "    std = float(lines[1].split()[1])\n",
    "        \n",
    "# Reverse standardization\n",
    "predictions = predictions * std + mean\n",
    "predictions = np.nan_to_num(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to ./data/output/predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Giả sử `test_data` có index là Id\n",
    "ids = test_data.index  # Lấy cột Id từ dữ liệu test ban đầu\n",
    "\n",
    "# Tạo DataFrame với 2 cột: Id và SalesPrice\n",
    "df_predictions = pd.DataFrame({'Id': ids, 'SalePrice': predictions.flatten()})\n",
    "\n",
    "# Lưu file CSV\n",
    "output_dir = './data/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_filepath = os.path.join(output_dir, 'predictions.csv')\n",
    "\n",
    "df_predictions.to_csv(output_filepath, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {output_filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
